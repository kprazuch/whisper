{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@gpj/diarising-audio-transcriptions-from-mp4s-with-python-and-whisper-a-step-by-step-guide-ebc9e6522e2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"file_path: data/jimmy_fallon_seth_meyers.mp3\"\n",
      "\"file_path_audio: data/jimmy_fallon_seth_meyers_0Q57FXSU.wav\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.0-full_build-www.gyan.dev Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 12.2.0 (Rev10, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-sdl2 --enable-libaribb24 --enable-libdav1d --enable-libdavs2 --enable-libuavs3d --enable-libzvbi --enable-librav1e --enable-libsvtav1 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs2 --enable-libxvid --enable-libaom --enable-libjxl --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-liblensfun --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-ffnvcodec --enable-nvdec --enable-nvenc --enable-d3d11va --enable-dxva2 --enable-libvpl --enable-libshaderc --enable-vulkan --enable-libplacebo --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libilbc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint\n",
      "  libavutil      58.  2.100 / 58.  2.100\n",
      "  libavcodec     60.  3.100 / 60.  3.100\n",
      "  libavformat    60.  3.100 / 60.  3.100\n",
      "  libavdevice    60.  1.100 / 60.  1.100\n",
      "  libavfilter     9.  3.100 /  9.  3.100\n",
      "  libswscale      7.  1.100 /  7.  1.100\n",
      "  libswresample   4. 10.100 /  4. 10.100\n",
      "  libpostproc    57.  1.100 / 57.  1.100\n",
      "Input #0, mp3, from 'data/jimmy_fallon_seth_meyers.mp3':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Lavf56.40.101\n",
      "  Duration: 00:05:19.32, start: 0.025057, bitrate: 128 kb/s\n",
      "  Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc56.60\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'data/jimmy_fallon_seth_meyers_0Q57FXSU.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 22050 Hz, mono, s16, 352 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc pcm_s16le\n",
      "size=       0kB time=-577014:32:22.77 bitrate=  -0.0kbits/s speed=N/A    \n",
      "size=   13751kB time=00:05:19.29 bitrate= 352.8kbits/s speed= 726x    \n",
      "video:0kB audio:13751kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000312%\n"
     ]
    }
   ],
   "source": [
    "# Define the file path to the Video\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "file_path = \"data/jimmy_fallon_seth_meyers.mp3\"\n",
    "# remove the .mp4 extension from the file path and add a random string of 8 characters to the end of the file path\n",
    "file_path_audio = file_path[:-4] + \"_\" + ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) + \".wav\"\n",
    "\n",
    "\n",
    "# Export both to os environment\n",
    "os.environ[\"file_path\"] = file_path\n",
    "os.environ[\"file_path_audio\"] = file_path_audio\n",
    "\n",
    "# set the file path by exporting the file path to the os environment using unix\n",
    "!echo \"file_path: %file_path%\"\n",
    "!echo \"file_path_audio: %file_path_audio%\"\n",
    "\n",
    "# Convert the video file_path to an audio mp4 file and be verbose\n",
    "!ffmpeg -i \"%file_path%\" -f wav -bitexact -acodec pcm_s16le -ar 22050 -ac 1 \"%file_path_audio%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 17.7M/17.7M [00:29<00:00, 595kB/s]\n",
      "(…)segmentation/resolve/2022.07/config.yaml: 100%|██████████| 318/318 [00:00<00:00, 309kB/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\KPRAZUCH\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.0.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)a-voxceleb/resolve/main/hyperparams.yaml: 100%|██████████| 1.92k/1.92k [00:00<00:00, 1.92MB/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 1314] A required privilege is not held by the client: 'C:\\\\Users\\\\KPRAZUCH\\\\.cache\\\\huggingface\\\\hub\\\\models--speechbrain--spkrec-ecapa-voxceleb\\\\snapshots\\\\5c0be3875fda05e81f3c004ed8c7c06be308de1e\\\\hyperparams.yaml' -> 'C:\\\\Users\\\\KPRAZUCH\\\\.cache\\\\torch\\\\pyannote\\\\speechbrain\\\\hyperparams.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Get the data from the model for speaker diarization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyannote\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[1;32m----> 4\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mpyannote/speaker-diarization\u001b[39;49m\u001b[39m'\u001b[39;49m, use_auth_token\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhf_NmjCtLZqZiQPBRBUrvMBgrklxGDezPOONB\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Get the audio file from the file path\u001b[39;00m\n\u001b[0;32m      7\u001b[0m diarization_file \u001b[39m=\u001b[39m file_path_audio \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_diarization.txt\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KPRAZUCH\\Documents\\whisper\\.venv\\lib\\site-packages\\pyannote\\audio\\core\\pipeline.py:136\u001b[0m, in \u001b[0;36mPipeline.from_pretrained\u001b[1;34m(cls, checkpoint_path, hparams_file, use_auth_token, cache_dir)\u001b[0m\n\u001b[0;32m    134\u001b[0m params \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mpipeline\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m, {})\n\u001b[0;32m    135\u001b[0m params\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39muse_auth_token\u001b[39m\u001b[39m\"\u001b[39m, use_auth_token)\n\u001b[1;32m--> 136\u001b[0m pipeline \u001b[39m=\u001b[39m Klass(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    138\u001b[0m \u001b[39m# freeze  parameters\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfreeze\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n",
      "File \u001b[1;32mc:\\Users\\KPRAZUCH\\Documents\\whisper\\.venv\\lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_diarization.py:165\u001b[0m, in \u001b[0;36mSpeakerDiarization.__init__\u001b[1;34m(self, segmentation, segmentation_step, embedding, embedding_exclude_overlap, clustering, embedding_batch_size, segmentation_batch_size, der_variant, use_auth_token)\u001b[0m\n\u001b[0;32m    162\u001b[0m     metric \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnot_applicable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding \u001b[39m=\u001b[39m PretrainedSpeakerEmbedding(\n\u001b[0;32m    166\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding, use_auth_token\u001b[39m=\u001b[39;49muse_auth_token\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_audio \u001b[39m=\u001b[39m Audio(sample_rate\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding\u001b[39m.\u001b[39msample_rate, mono\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdownmix\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m     metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding\u001b[39m.\u001b[39mmetric\n",
      "File \u001b[1;32mc:\\Users\\KPRAZUCH\\Documents\\whisper\\.venv\\lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:748\u001b[0m, in \u001b[0;36mPretrainedSpeakerEmbedding\u001b[1;34m(embedding, device, use_auth_token)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Pretrained speaker embedding\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \n\u001b[0;32m    718\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[39m>>> embeddings = get_embedding(waveforms, masks=masks)\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(embedding, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mspeechbrain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m embedding:\n\u001b[1;32m--> 748\u001b[0m     \u001b[39mreturn\u001b[39;00m SpeechBrainPretrainedSpeakerEmbedding(\n\u001b[0;32m    749\u001b[0m         embedding, device\u001b[39m=\u001b[39;49mdevice, use_auth_token\u001b[39m=\u001b[39;49muse_auth_token\n\u001b[0;32m    750\u001b[0m     )\n\u001b[0;32m    752\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(embedding, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnvidia\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m embedding:\n\u001b[0;32m    753\u001b[0m     \u001b[39mreturn\u001b[39;00m NeMoPretrainedSpeakerEmbedding(embedding, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\KPRAZUCH\\Documents\\whisper\\.venv\\lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:260\u001b[0m, in \u001b[0;36mSpeechBrainPretrainedSpeakerEmbedding.__init__\u001b[1;34m(self, embedding, device, use_auth_token)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    258\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[1;32m--> 260\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier_ \u001b[39m=\u001b[39m SpeechBrain_EncoderClassifier\u001b[39m.\u001b[39;49mfrom_hparams(\n\u001b[0;32m    261\u001b[0m     source\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding,\n\u001b[0;32m    262\u001b[0m     savedir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mCACHE_DIR\u001b[39m}\u001b[39;49;00m\u001b[39m/speechbrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    263\u001b[0m     run_opts\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice},\n\u001b[0;32m    264\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[0;32m    265\u001b[0m     revision\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrevision,\n\u001b[0;32m    266\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\KPRAZUCH\\Documents\\whisper\\.venv\\lib\\site-packages\\speechbrain\\pretrained\\interfaces.py:392\u001b[0m, in \u001b[0;36mPretrained.from_hparams\u001b[1;34m(cls, source, hparams_file, pymodule_file, overrides, savedir, use_auth_token, revision, download_only, **kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m     clsname \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m    391\u001b[0m     savedir \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./pretrained_models/\u001b[39m\u001b[39m{\u001b[39;00mclsname\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mhashlib\u001b[39m.\u001b[39mmd5(source\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mUTF-8\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39merrors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mhexdigest()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 392\u001b[0m hparams_local_path \u001b[39m=\u001b[39m fetch(\n\u001b[0;32m    393\u001b[0m     filename\u001b[39m=\u001b[39;49mhparams_file,\n\u001b[0;32m    394\u001b[0m     source\u001b[39m=\u001b[39;49msource,\n\u001b[0;32m    395\u001b[0m     savedir\u001b[39m=\u001b[39;49msavedir,\n\u001b[0;32m    396\u001b[0m     overwrite\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    397\u001b[0m     save_filename\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    398\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    399\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    400\u001b[0m )\n\u001b[0;32m    401\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     pymodule_local_path \u001b[39m=\u001b[39m fetch(\n\u001b[0;32m    403\u001b[0m         filename\u001b[39m=\u001b[39mpymodule_file,\n\u001b[0;32m    404\u001b[0m         source\u001b[39m=\u001b[39msource,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    409\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m    410\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\KPRAZUCH\\Documents\\whisper\\.venv\\lib\\site-packages\\speechbrain\\pretrained\\fetching.py:178\u001b[0m, in \u001b[0;36mfetch\u001b[1;34m(filename, source, savedir, overwrite, save_filename, use_auth_token, revision, cache_dir, silent_local_fetch)\u001b[0m\n\u001b[0;32m    176\u001b[0m     sourcepath \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(fetched_file)\u001b[39m.\u001b[39mabsolute()\n\u001b[0;32m    177\u001b[0m     _missing_ok_unlink(destination)\n\u001b[1;32m--> 178\u001b[0m     destination\u001b[39m.\u001b[39;49msymlink_to(sourcepath)\n\u001b[0;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m destination\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\pathlib.py:1253\u001b[0m, in \u001b[0;36mPath.symlink_to\u001b[1;34m(self, target, target_is_directory)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msymlink_to\u001b[39m(\u001b[39mself\u001b[39m, target, target_is_directory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1249\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[39m    Make this path a symlink pointing to the target path.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[39m    Note the order of arguments (link, target) is the reverse of os.symlink.\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49msymlink(target, \u001b[39mself\u001b[39;49m, target_is_directory)\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1314] A required privilege is not held by the client: 'C:\\\\Users\\\\KPRAZUCH\\\\.cache\\\\huggingface\\\\hub\\\\models--speechbrain--spkrec-ecapa-voxceleb\\\\snapshots\\\\5c0be3875fda05e81f3c004ed8c7c06be308de1e\\\\hyperparams.yaml' -> 'C:\\\\Users\\\\KPRAZUCH\\\\.cache\\\\torch\\\\pyannote\\\\speechbrain\\\\hyperparams.yaml'"
     ]
    }
   ],
   "source": [
    " # Get the data from the model for speaker diarization\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization')\n",
    "\n",
    "# Get the audio file from the file path\n",
    "diarization_file = file_path_audio + \"_diarization.txt\"\n",
    "\n",
    "# If the file exists, then read the file and return the data\n",
    "if os.path.exists(diarization_file):\n",
    "    with open(diarization_file, \"r\") as text_file:\n",
    "        dz = str(text_file.read())\n",
    "else:\n",
    "    # If the file does not exist, then get the data from the model\n",
    "    AUDIO_FILE = {'uri': 'blabla', 'audio': file_path_audio}\n",
    "    dz = pipeline(AUDIO_FILE)  \n",
    "\n",
    "    # Use the file path to save the file to a text file with {filepath}_diarization.txt\n",
    "    with open(diarization_file, \"w\") as text_file:\n",
    "        text_file.write(str(dz))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
